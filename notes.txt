What is a Neuron?
--> A neuron is a combination of linear and logistic regression i.e. its first part is a linear eqn (weighted sum) and its second part is a sigmoid function which is also called as an activation function
--> Here the logic is that, we have an input and linear eqn will give a continuous value and that continuous value is converted to 0/1 using activation or sigmoid function
--> If the value is 1, we say that the neuron is activated else we say that it is deactivated


What is a Neural Network?
--> Each neuron is given a specific subtask of the whole task (e.g. if we need to identify if the given image is of a koala or not, each neuron will work on a specific part of the koala's body such as ears, nose, eyes, legs, etc) and these parts will gave their own weightage in the identification
--> These neurons form groups. A group tells its results to the next group and so on
--> there might be 3 groups at level 1, 2 groups at level 2 and then 1 group at level 3
--> Intermediate groups are known as the hidden layer
--> Structure =         Input Layer ==>  Hidden Layer ==>  Output Layer


Training of a Neural Network:
--> Initially, one image of koala will be shown to all the neurons and they will make a random guess whether that subpart looks like a koala's or not and based on that an output will be generated
--> That output will be handed over to a supervisor who knows the correct answer and that correct answer will again be handed over to output layer
--> The correct answer then flows from output layer to hidden layer and then to the neurons and these neurons adjust their weights accordingly
--> These neurons learn from their mistakes if they predicted it wrong and this process is called Backward Error Propagation
--> This process is repeated n number of times (again they make a random guess but this time considering the past mistakes)
--> Initially, the neurons will make a lot of errors but with time, they keep improving
--> For this improvement to be done by the neurons, some mathematics and derivatives are used
--> The motivation for this came from how a human brain works => consider the example of learning a bicycle => The same process of propagation goes on in our neurons
--> The main thing about the neural network is that the neurons are naive about what subpart are they going to work on => there is a mathematics involved in that part using which each neuron is able to decide what subtask it needs to work on


PyTorch v/s Tensorflow v/s Keras
--> PyTorch and Tensorflow are the two frameworks for Deep learning
--> There's another one called CNTK but that one is not used much
--> Keras is a wrapper around Tensorflow, CNTK and theano which provides convinience (It has its backed by default as Tensorflow but it can be changed to others)
--> In Tensorflow 2.0, keras is made a part of the library itself


e.g. 

Input Layer      Hidden Layer        Output Layer                   => Here if every neuron will be connected with every neuron in the
                                                                       hidden layer ==> dense layer
1) Age  -------> Awareness
2) Education -->
                                        Yes/No
3) Income -----> Affordability
4) Savings ---->



Handwritten Digits Classification

-->Input Neuron => Hidden Layer => Output Neuron (1-9 digits' probability or value between 0 to 1)
--> But in this case, what would be the input features
--> Get the 2D matrix of pixels (let's say 7x7) from the image where the values will range from 0 to 255 (0=black and 255=white)
--> Convert the 2D array into 1D array to get 7*7=49 input features
--> There won't be a hidden layer in this case but adding it improves the accuracy


Importance of Activation Functions and its types
--> Activation functions are required to make the process of Classification easy as it converts the output in the range 0 to 1
--> It is hard to classify the output if it is a continuous variable hence it needs to be converted into a value between 0 to 1
--> Hence activation function is helpful in the output layer
--> Also in the case of hidden layers, the activation function is ised alongside the weighted sum of the linear equation
--> If we have 5 hidden layers and we remove the activation function from all these layers, then we'd realise that we don't need activation function as all...The hidden layers won't affect the output
--> Just the linear equations are not enough to solve a problem and activation function allows us to convert it in a non-linear form

1) Step and Sigmoid Functions
--> For the Classification, we can use step function i.e. if the value is >0.5, it is 1 else it is 0
--> But the problem is that it is affected by the outliers
--> Another problem is that it gives values as only 0 and 1 and nothing in between them
--> Suppose, we need to classify handwritten digits and the value for 2 digits comes out to be >0.5 and hence they will be treated as 1
--> So it is impossible to decide which one is the correct handwritten digit
--> That's why sigmoid function is used which gives us the smooth curve and hence the maximum value can be used to get the output

2) tanh Function
--> It is similar to sigmoid function but it gives an output value between -1 to 1

--> The general guideline is that always use sigmoid in the output layer and in all other places, try to use tanh
--> Because tanh will try to center your data

--> Derivative tells the slope i.e. how much the output changes for a given input change
--> But it creates a problem in the learning process as its value is 0 for +ve and -ve higher values => if your derivates are closer to 0, the learning becomes extremely slow (Vanishing Gradient Problem) => the logic is that in gradient descent, we calculate the derivative and then backpropagate the arrow

--> To overcome this problem, a new function exists known as ReLU
--> If you value is less than 0 then output is 0, else the output is same as that value (45 degree line)
--> output = max(0,x)
--> The guideline is that for hidden layers, if you're not sure which activation function to use, just use relu as a default choice
--> ReLU is a lightweight function and computationally very effective
--> ReLU also has the Vanishing Gradient Problem as for value<0, derivative is 0
--> For that there is a variation of ReLU called Leaky ReLU in which the output = max(0.1x,x)



Derivatives for Algorithms
--> Algorithms like Gradient Descent and Backward Propagation require the understanding of derivatives and partial derivatives
--> Derivative is the slope of the line
--> In case of a linear line, it is easy to calculate but in case of a non-linear line, it is difficult to calculate the slope
--> Hence we zoom into the line and we can see an almost straight line => slope can be calculated and it doesn't come out to be a constant but a function of x
--> For partial derivates, if we need to find the derivative wrt x, we consider y as a constant
--> In case of Deep Learning, suppose we have bedrooms and area as our features and we need to predict the price => del(Price)/del(bedrooms) tells us "How much a price is changing given a change in bedrooms"
--> In neural networks, the initial weights are considered to be 1 and based on trial and error the weights are adjusted to improve the accuracy => NEURAL NETWRORK TRAINING IS ALL ABOUT ADJUSTING WEIGHTS
--> To get the weights properly adjusted and not get caught into an infinite loop, derivates are used
--> Suppose => we take the derivative of error wrt to Age (How much an error/output changes based on the change in age) => It is represented by the concept of partial derivative wrt all the features in the problem



Matrix Operations in Deep Learning
--> Various operations such as matrix multiplication, dot product, etc are used in Deep Learning
--> Numpy can be used to aid these operations which is a special library for matrix manipulations



Loss Functions or Cost functions
--> Loss functions are used in neural network training
--> 5 loss functions can be used

e.g. Cards => K,9,7     Predicted => 8,Q,6
1) Mean Absolute Error => (K-8)+(Q-9)+(7-6) = 5+3+1 = 9/3 = 3       => /3 for mean
2) Mean Sqaured Error => (K-8)^2+(Q-9)^2+(7-6)^2 = 25+9+1 = 35/3    => /3 for mean
   --> Having Sqaured error in ML gives you some utility => It allows to converge your Gradient Descent in a better way

Why Loss functions =>
--> e.g. Based on age and Affordability, we are predicting if a person will buy insurance or not
--> age, aff = x and buy = y => y = f(x) = w1x1 + w2x2
--> We randomly assign values to w1 and w2 for training and we find the outcome => passing the values is called a forward pass
--> Let's say y'=0.99 but y=0 => Absolute error = 0.99.... Also if y'=0.99 and y=1 => Absolute error = 0.01
--> Then take the avg of all the errors of the samples => Mean Absolute error
--> Mean Absolute error = cost function and each individual error = Loss
--> Here, for 13 samples, we get MAE = 10.02
--> Once we go through 'one round' of all the training samples => it is called as 1 epoch

3) log loss or binary crossentropy = -1/n sum(yi*log(y'i)+(1-yi)*log(1-yi))
--> Practically, for logistic regression, we use log loss => why? => https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c



Gradient Descent
--> It is used to find the linear equation given the values of two variables => It finds the relationship between two or more variables => also called prediction function
--> y = mx+c => m = weight, c = bias
--> Gradient Descent is used during the training of a neural network
--> Same e.g. as above => predict if the person will buy insurance or not based on age and Affordability for 13 samples
--> You randomly assign the weights(1) and bias(0) initially and do the forward passing for the 1st samples
--> You get the outcome and calculate the error => let's say you calculate the log loss (for logistic)
--> After calculating the errors for all 13 samples, you get the total error and find out the final log loss or binary cross entropy
--> This process is called one epoch
--> Now, your goal is to backpropagate this loss such that the weights will be adjusted => the main goal is to adjust these weights such that the log loss will be reduced
--> Now, w1 = w1-something
--> Derivative of your loss wrt w1 indicates how much the log loss changes for a given change in w1
--> w1 = w1 - (learning rate)*d/dw1 => learning weight is a small value (generally 0.1) so that you don't adjust your weight drastically
--> same for w2 and b (bias)
--> Now, d/dw1 = 1/n sum(xi(y'i-yi))
--> For bias,  d/db = 1/n sum(y'i-yi)
--> After calculating these values, we get w1 = 0.8, w2 = 0.7 and b = 0.2
--> You again repeat the same process => from forwrd passing to getting the log loss
--> By the way, here we are using batch gradient => you need to feed the entire dataset for one epoch (other two types are stochastic and mini-batch gradient)
--> Now, we keep on getting different values of errors and try to minimize it in each epoch...But when should be stop?
--> If your loss function is a convex function (y-axis=loss, x-axis=w1, z-axis=b) => gradient like graph but 3d
--> You start with a random value and your loss is high
--> By calculating the derivative, you are constantly moving towards the minimum
--> Once you are at the minimum point of loss, you get the values for weights and bias => once you find these values, you have your prediction function
--> Gradient descent allows you to do the same
--> You constantly move from a higher point to a global minima and while moving, you draw a tangent at every point and that is the derivative at that point => you multiply that derivative with the learning rate and move towards the global minima



Flow of Neural Networks:
--> Firstly, there are two functions which we need to call/perform, fit and predict
--> When we call the fit function, the call goes to gradient descent to find weights and bias => Hence, model is trained
--> After training, we call the predict function
--> In predict function, we get the weighted sum by w1*X_test['age']+w2*X_test['affordability']+bias and use sigmoid_numpy() to get the final output
